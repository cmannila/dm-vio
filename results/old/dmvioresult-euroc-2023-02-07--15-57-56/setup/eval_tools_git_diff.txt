diff --git a/configs.yaml b/configs.yaml
index 87fa7f2..dee41ad 100644
--- a/configs.yaml
+++ b/configs.yaml
@@ -1,3 +1,16 @@
+workpc:
+  short_name: workpc
+  dmvio_folder: /home/cm2113/workspace/dm-vio
+  pc_config_path:
+  pc_config_command: apt list
+  slurm: false
+  results_path: /home/cm2113/workspace/dm-vio/results
+  euroc:
+    dataset_path: /home/cm2113/workspace/Datasets/euroc
+    results_path: /home/cm2113/workspace/dm-vio/results
+  tumvi:
+    dataset_path: /home/cm2113/workspace/Datasets/tumvi
+    results_path: /home/cm2113/workspace/dm-vio/results
 example_config:
   short_name: example
   dmvio_folder: /path/to/dmvio
@@ -11,8 +24,8 @@ example_config:
   slurm: false # Can be set to true for running on Slurm servers.
   results_path: /path/where/results/are/stored # Your results will be stored (and read from) here.
   euroc: # Paths to the individual datasets.
-    dataset_path: /path/to/euroc/dataset
-    results_path: /path/where/results/are/stored
+    dataset_path: /home/cm2113/workspace/Datasets/euroc
+    results_path: /home/cm2113/workspace/dm-vio/results
   tumvi:
     dataset_path: /path/to/tumvi/dataset
     results_path: /path/where/results/are/stored
diff --git a/run_dmvio.py b/run_dmvio.py
index 856bc38..b38354d 100644
--- a/run_dmvio.py
+++ b/run_dmvio.py
@@ -335,4 +335,4 @@ def build_results_name(name, realtime, dataset):
 
 
 if __name__ == "__main__":
-    main()
+    main()
\ No newline at end of file
diff --git a/trajectory_evaluation/evaluate.py b/trajectory_evaluation/evaluate.py
index f5b8782..3c9c4d5 100644
--- a/trajectory_evaluation/evaluate.py
+++ b/trajectory_evaluation/evaluate.py
@@ -114,11 +114,10 @@ def evaluate_with_config(pair, always_reevaluate=False):
     return evaluate_run(folder, dataset, setup['num_iter'], None, always_reevaluate)
 
 
-def evaluate_run(run_folder: Path, dataset: Dataset, num_iter: int, name=None, always_reevaluate=False) -> (
-        EvalResults, EvalResults):
+def evaluate_run(run_folder: Path, dataset: Dataset, num_iter: int, name=None, always_reevaluate=False) -> (EvalResults, EvalResults):
     """Evaluate all sequences and iterations of a run and save it to file (and return it).
     If the evaluation result has already been saved to file it will just load it.
-        returns
+        retu
 
     :param run_folder: Folder of the run which will be evaluated.
     :param dataset: Dataset to evaluate on.
